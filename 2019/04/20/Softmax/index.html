<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>原理就是这么简单- Softmax | “云雾中的墨先生”</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一般的神经网络课程在讲述BP算法的时候都会用sigmoid 函数来作为最后一层的激活函数来实现二分类，这是因为其sigmoid求导的便利，非常适合用来讲课（吴大大的课也是如此）。 但是对于多分类的问题很多小伙伴表示喜欢直接调库。其实了解softmax反向传播原理，或者可以自己手推一遍softmax的BP算法，才能彰显一个AI从业人员的内功。 此文会通过一个案例为大家一步一步讲解softmax 反向">
<meta name="keywords" content="Deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="原理就是这么简单- Softmax">
<meta property="og:url" content="http://clouder.com/2019/04/20/Softmax/index.html">
<meta property="og:site_name" content="“云雾中的墨先生”">
<meta property="og:description" content="一般的神经网络课程在讲述BP算法的时候都会用sigmoid 函数来作为最后一层的激活函数来实现二分类，这是因为其sigmoid求导的便利，非常适合用来讲课（吴大大的课也是如此）。 但是对于多分类的问题很多小伙伴表示喜欢直接调库。其实了解softmax反向传播原理，或者可以自己手推一遍softmax的BP算法，才能彰显一个AI从业人员的内功。 此文会通过一个案例为大家一步一步讲解softmax 反向">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-6bcc60635cf2a0a2.png!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-08721672a3b926fc.png!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-84c5124b7f76696b.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-eb884a1c20874926.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-1f3e71770ee5328d.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-a7b9b2ac32c0c3d3.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-150f68cad25cc132.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-693f8cefe7ff1a5f.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-65b08d5148fbd32b.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-bf882cd01ce89473.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-4da9affc4120bf84.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-e9e127c4f8f2dd37.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4861716-b2511bc623898e0f.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-20T23:53:06.985Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="原理就是这么简单- Softmax">
<meta name="twitter:description" content="一般的神经网络课程在讲述BP算法的时候都会用sigmoid 函数来作为最后一层的激活函数来实现二分类，这是因为其sigmoid求导的便利，非常适合用来讲课（吴大大的课也是如此）。 但是对于多分类的问题很多小伙伴表示喜欢直接调库。其实了解softmax反向传播原理，或者可以自己手推一遍softmax的BP算法，才能彰显一个AI从业人员的内功。 此文会通过一个案例为大家一步一步讲解softmax 反向">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/4861716-6bcc60635cf2a0a2.png!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
    <link rel="alternate" href="/atom.xml" title="“云雾中的墨先生”" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">“云雾中的墨先生”</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">只想告诉你最简单的原理</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://clouder.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Softmax" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/20/Softmax/" class="article-date">
  <time datetime="2019-04-20T15:29:39.000Z" itemprop="datePublished">2019-04-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      原理就是这么简单- Softmax
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一般的神经网络课程在讲述BP算法的时候都会用sigmoid 函数来作为最后一层的激活函数来实现二分类，这是因为其sigmoid求导的便利，非常适合用来讲课（吴大大的课也是如此）。</p>
<p>但是对于多分类的问题很多小伙伴表示喜欢直接调库。其实了解softmax反向传播原理，或者可以自己手推一遍softmax的BP算法，才能彰显一个AI从业人员的内功。</p>
<p>此文会通过一个案例为大家一步一步讲解softmax 反向传播的多个知识点，只要仔细看完此文你会对整个流程豁然开朗。</p>
<p>现有如下的分类问题，可以通过动物的4个特征来区分此动物到底是什么种类（猫，狗，人 3个类别）</p>
<p>我们可以建立如下NN模型来解决这个问题：</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-6bcc60635cf2a0a2.png!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>在开始讲解之前，首先定义一下此文的统一符号命名,(很重要)</p>
<p>红色的是Input 简写为</p>
<p>蓝色的是Output 简写为 O</p>
<p>红色和蓝色之间的是权重矩阵 简写为 W</p>
<p>绿色的是Output经过Softmax之后的结果 简写为 yhat</p>
<p><strong>1 、正向传播：</strong></p>
<p>O = X W</p>
<p>yhat = Softmax(O)</p>
<p>@﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-08721672a3b926fc.png!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿此时softmax 有三个神经元输出 m = 3</p>
<p><em>重要</em>：</p>
<p>X (Shape 为（1,4）的向量)</p>
<p>W （Shape为（4,3）的矩阵）</p>
<p>yhat （Shape 为（1,3）的向量）</p>
<p><strong>2、反向传播</strong></p>
<p><strong>1) 求损失</strong></p>
<p>因为是分类问题,我们采用交叉熵作为 loss function（关于为什么交叉熵可以作为损失函数我会在后期写一篇文章进行介绍，敬请期待）, 如下：</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-84c5124b7f76696b.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p><strong>2) 求梯度</strong></p>
<p>我先给出梯度公式也就是项目中的error-term :</p>
<p><strong>−<em>x</em>⋅(<em>yhat</em>−<em>y)</em> </strong></p>
<p>想必各位想知道这个公式到底怎么推导得到的呢？</p>
<p>接下来我会讲解此文的重点</p>
<p><em>重要</em> 求权重的梯度需要运用<strong>链式求导法则</strong>，如下：</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-eb884a1c20874926.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>我们来分别计算后面的三项：</p>
<p><strong><em>第一项：</em></strong></p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-1f3e71770ee5328d.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p><strong><em>第二项：</em></strong></p>
<p>稍微有点复杂。</p>
<p>为什么yhat下标为i 而O下标为j 呢？</p>
<p>这是由于softmax 函数的分母有多个输出，但是针对yhati 需要计算所有的softmax</p>
<p>由于softmax有多个输出，所以需要考虑 i = j 与 i != j 两种情形。</p>
<p>当 i != j 时：</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-a7b9b2ac32c0c3d3.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>所以可得结果为： ﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-150f68cad25cc132.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>当 i = j 时：</p>
<p>将 j 替换为 i如下</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-693f8cefe7ff1a5f.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>所以可得 结果为：﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-65b08d5148fbd32b.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p><strong><em>第三项：</em></strong></p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-bf882cd01ce89473.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>第三项非常简单，我们先来算前两项</p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-4da9affc4120bf84.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>到这里别忘了 我们经过softmax 的输出只有一个类别也就是 yi = 1, 其他的都等于0</p>
<p>所以前两项的结果为：﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-e9e127c4f8f2dd37.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p><strong><em>最终我们就得到了结果：</em></strong></p>
<p>﻿<img src="http://upload-images.jianshu.io/upload_images/4861716-b2511bc623898e0f.PNG!thumbnail?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"> ﻿</p>
<p>但是Error - term 为 <strong>−<em>x</em>⋅(<em>yhat</em>−<em>y)</em> </strong></p>
<p>这里为什么有个负号？</p>
<p>是因为模型应该朝着负梯度的方向优化，所以加了一个负号。</p>
<p>到此softmax 的正向与反向传播都讲清楚了，欢迎支持 《原理就是这么简单系列》</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://clouder.com/2019/04/20/Softmax/" data-id="cjuq5sn770000v4vijem9ey2d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-learning/">Deep learning</a></li></ul>

    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-learning/">Deep learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/04/20/Softmax/">原理就是这么简单- Softmax</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Clouder<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>